{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving N-Grams from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [N-Gram-Based Text Categorization: Categorizing Text With Python by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)\n",
    "\n",
    "What are n-grams? See [here](http://cloudmark.github.io/Language-Detection/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
    "s = s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le',\n",
       " 'temps',\n",
       " 'est',\n",
       " 'un',\n",
       " 'grand',\n",
       " 'maître',\n",
       " 'dit',\n",
       " 'on',\n",
       " 'le',\n",
       " 'malheur',\n",
       " 'est',\n",
       " \"qu'il\",\n",
       " 'tue',\n",
       " 'ses',\n",
       " 'élèves']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
    "s_tokenized = tokenizer.tokenize(s)\n",
    "s_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'e'),\n",
       "  ('_', 't', 'e', 'm'),\n",
       "  ('t', 'e', 'm', 'p'),\n",
       "  ('e', 'm', 'p', 's'),\n",
       "  ('m', 'p', 's', '_'),\n",
       "  ('p', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'u'),\n",
       "  ('_', '_', 'u', 'n'),\n",
       "  ('_', 'u', 'n', '_'),\n",
       "  ('u', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'g'),\n",
       "  ('_', '_', 'g', 'r'),\n",
       "  ('_', 'g', 'r', 'a'),\n",
       "  ('g', 'r', 'a', 'n'),\n",
       "  ('r', 'a', 'n', 'd'),\n",
       "  ('a', 'n', 'd', '_'),\n",
       "  ('n', 'd', '_', '_'),\n",
       "  ('d', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'î'),\n",
       "  ('m', 'a', 'î', 't'),\n",
       "  ('a', 'î', 't', 'r'),\n",
       "  ('î', 't', 'r', 'e'),\n",
       "  ('t', 'r', 'e', '_'),\n",
       "  ('r', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'd'),\n",
       "  ('_', '_', 'd', 'i'),\n",
       "  ('_', 'd', 'i', 't'),\n",
       "  ('d', 'i', 't', '_'),\n",
       "  ('i', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'o'),\n",
       "  ('_', '_', 'o', 'n'),\n",
       "  ('_', 'o', 'n', '_'),\n",
       "  ('o', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'l'),\n",
       "  ('m', 'a', 'l', 'h'),\n",
       "  ('a', 'l', 'h', 'e'),\n",
       "  ('l', 'h', 'e', 'u'),\n",
       "  ('h', 'e', 'u', 'r'),\n",
       "  ('e', 'u', 'r', '_'),\n",
       "  ('u', 'r', '_', '_'),\n",
       "  ('r', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'q'),\n",
       "  ('_', '_', 'q', 'u'),\n",
       "  ('_', 'q', 'u', \"'\"),\n",
       "  ('q', 'u', \"'\", 'i'),\n",
       "  ('u', \"'\", 'i', 'l'),\n",
       "  (\"'\", 'i', 'l', '_'),\n",
       "  ('i', 'l', '_', '_'),\n",
       "  ('l', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'u'),\n",
       "  ('_', 't', 'u', 'e'),\n",
       "  ('t', 'u', 'e', '_'),\n",
       "  ('u', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 's'),\n",
       "  ('_', '_', 's', 'e'),\n",
       "  ('_', 's', 'e', 's'),\n",
       "  ('s', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'é'),\n",
       "  ('_', '_', 'é', 'l'),\n",
       "  ('_', 'é', 'l', 'è'),\n",
       "  ('é', 'l', 'è', 'v'),\n",
       "  ('l', 'è', 'v', 'e'),\n",
       "  ('è', 'v', 'e', 's'),\n",
       "  ('v', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "generated_4grams = []\n",
    "\n",
    "for word in s_tokenized:\n",
    "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
    "generated_4grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `generated_4grams` needs flattening since it's supposed to be a list of 4-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', '_', '_', 'l'),\n",
       " ('_', '_', 'l', 'e'),\n",
       " ('_', 'l', 'e', '_'),\n",
       " ('l', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 't'),\n",
       " ('_', '_', 't', 'e'),\n",
       " ('_', 't', 'e', 'm'),\n",
       " ('t', 'e', 'm', 'p'),\n",
       " ('e', 'm', 'p', 's')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
    "generated_4grams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining n-grams (n = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['___l',\n",
       " '__le',\n",
       " '_le_',\n",
       " 'le__',\n",
       " 'e___',\n",
       " '___t',\n",
       " '__te',\n",
       " '_tem',\n",
       " 'temp',\n",
       " 'emps',\n",
       " 'mps_',\n",
       " 'ps__',\n",
       " 's___',\n",
       " '___e',\n",
       " '__es',\n",
       " '_est',\n",
       " 'est_',\n",
       " 'st__',\n",
       " 't___',\n",
       " '___u',\n",
       " '__un',\n",
       " '_un_',\n",
       " 'un__',\n",
       " 'n___',\n",
       " '___g',\n",
       " '__gr',\n",
       " '_gra',\n",
       " 'gran',\n",
       " 'rand',\n",
       " 'and_',\n",
       " 'nd__',\n",
       " 'd___',\n",
       " '___m',\n",
       " '__ma',\n",
       " '_maî',\n",
       " 'maît',\n",
       " 'aîtr',\n",
       " 'ître',\n",
       " 'tre_',\n",
       " 're__',\n",
       " 'e___',\n",
       " '___d',\n",
       " '__di',\n",
       " '_dit',\n",
       " 'dit_',\n",
       " 'it__',\n",
       " 't___',\n",
       " '___o',\n",
       " '__on',\n",
       " '_on_',\n",
       " 'on__',\n",
       " 'n___',\n",
       " '___l',\n",
       " '__le',\n",
       " '_le_',\n",
       " 'le__',\n",
       " 'e___',\n",
       " '___m',\n",
       " '__ma',\n",
       " '_mal',\n",
       " 'malh',\n",
       " 'alhe',\n",
       " 'lheu',\n",
       " 'heur',\n",
       " 'eur_',\n",
       " 'ur__',\n",
       " 'r___',\n",
       " '___e',\n",
       " '__es',\n",
       " '_est',\n",
       " 'est_',\n",
       " 'st__',\n",
       " 't___',\n",
       " '___q',\n",
       " '__qu',\n",
       " \"_qu'\",\n",
       " \"qu'i\",\n",
       " \"u'il\",\n",
       " \"'il_\",\n",
       " 'il__',\n",
       " 'l___',\n",
       " '___t',\n",
       " '__tu',\n",
       " '_tue',\n",
       " 'tue_',\n",
       " 'ue__',\n",
       " 'e___',\n",
       " '___s',\n",
       " '__se',\n",
       " '_ses',\n",
       " 'ses_',\n",
       " 'es__',\n",
       " 's___',\n",
       " '___é',\n",
       " '__él',\n",
       " '_élè',\n",
       " 'élèv',\n",
       " 'lève',\n",
       " 'èves',\n",
       " 'ves_',\n",
       " 'es__',\n",
       " 's___']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_list_4grams = generated_4grams\n",
    "for idx, val in enumerate(generated_4grams):\n",
    "    ng_list_4grams[idx] = ''.join(val)\n",
    "ng_list_4grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sorting n-grams by frequency (n = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e___', 4),\n",
       " ('s___', 3),\n",
       " ('t___', 3),\n",
       " ('___l', 2),\n",
       " ('__le', 2),\n",
       " ('_le_', 2),\n",
       " ('le__', 2),\n",
       " ('___t', 2),\n",
       " ('___e', 2),\n",
       " ('__es', 2),\n",
       " ('_est', 2),\n",
       " ('est_', 2),\n",
       " ('st__', 2),\n",
       " ('n___', 2),\n",
       " ('___m', 2),\n",
       " ('__ma', 2),\n",
       " ('es__', 2),\n",
       " ('__te', 1),\n",
       " ('_tem', 1),\n",
       " ('temp', 1),\n",
       " ('emps', 1),\n",
       " ('mps_', 1),\n",
       " ('ps__', 1),\n",
       " ('___u', 1),\n",
       " ('__un', 1),\n",
       " ('_un_', 1),\n",
       " ('un__', 1),\n",
       " ('___g', 1),\n",
       " ('__gr', 1),\n",
       " ('_gra', 1),\n",
       " ('gran', 1),\n",
       " ('rand', 1),\n",
       " ('and_', 1),\n",
       " ('nd__', 1),\n",
       " ('d___', 1),\n",
       " ('_maî', 1),\n",
       " ('maît', 1),\n",
       " ('aîtr', 1),\n",
       " ('ître', 1),\n",
       " ('tre_', 1),\n",
       " ('re__', 1),\n",
       " ('___d', 1),\n",
       " ('__di', 1),\n",
       " ('_dit', 1),\n",
       " ('dit_', 1),\n",
       " ('it__', 1),\n",
       " ('___o', 1),\n",
       " ('__on', 1),\n",
       " ('_on_', 1),\n",
       " ('on__', 1),\n",
       " ('_mal', 1),\n",
       " ('malh', 1),\n",
       " ('alhe', 1),\n",
       " ('lheu', 1),\n",
       " ('heur', 1),\n",
       " ('eur_', 1),\n",
       " ('ur__', 1),\n",
       " ('r___', 1),\n",
       " ('___q', 1),\n",
       " ('__qu', 1),\n",
       " (\"_qu'\", 1),\n",
       " (\"qu'i\", 1),\n",
       " (\"u'il\", 1),\n",
       " (\"'il_\", 1),\n",
       " ('il__', 1),\n",
       " ('l___', 1),\n",
       " ('__tu', 1),\n",
       " ('_tue', 1),\n",
       " ('tue_', 1),\n",
       " ('ue__', 1),\n",
       " ('___s', 1),\n",
       " ('__se', 1),\n",
       " ('_ses', 1),\n",
       " ('ses_', 1),\n",
       " ('___é', 1),\n",
       " ('__él', 1),\n",
       " ('_élè', 1),\n",
       " ('élèv', 1),\n",
       " ('lève', 1),\n",
       " ('èves', 1),\n",
       " ('ves_', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_4grams = {}\n",
    "\n",
    "for ngram in ng_list_4grams:\n",
    "    if ngram not in freq_4grams:\n",
    "        freq_4grams.update({ngram: 1})\n",
    "    else:\n",
    "        ngram_occurrences = freq_4grams[ngram]\n",
    "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
    "        \n",
    "from operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
    "\n",
    "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] # We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
    "freq_4grams_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtaining n-grams for multiple values of n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get n-grams for n = 1, 2, 3 and 4 we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maître dit on le malheur est qu'il tue ses élèves\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import everygrams\n",
    "\n",
    "s_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n",
    "s_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'e',\n",
       " 'e_',\n",
       " '_t',\n",
       " '_te',\n",
       " '_tem',\n",
       " 't',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temp',\n",
       " 'e',\n",
       " 'em',\n",
       " 'emp',\n",
       " 'emps',\n",
       " 'm',\n",
       " 'mp',\n",
       " 'mps',\n",
       " 'mps_',\n",
       " 'p',\n",
       " 'ps',\n",
       " 'ps_',\n",
       " 's',\n",
       " 's_',\n",
       " '_e',\n",
       " '_es',\n",
       " '_est',\n",
       " 'e',\n",
       " 'es',\n",
       " 'est',\n",
       " 'est_',\n",
       " 's',\n",
       " 'st',\n",
       " 'st_',\n",
       " 't',\n",
       " 't_',\n",
       " '_u',\n",
       " '_un',\n",
       " '_un_',\n",
       " 'u',\n",
       " 'un',\n",
       " 'un_',\n",
       " 'n',\n",
       " 'n_',\n",
       " '_g',\n",
       " '_gr',\n",
       " '_gra',\n",
       " 'g',\n",
       " 'gr',\n",
       " 'gra',\n",
       " 'gran',\n",
       " 'r',\n",
       " 'ra',\n",
       " 'ran',\n",
       " 'rand',\n",
       " 'a',\n",
       " 'an',\n",
       " 'and',\n",
       " 'and_',\n",
       " 'n',\n",
       " 'nd',\n",
       " 'nd_',\n",
       " 'd',\n",
       " 'd_',\n",
       " '_m',\n",
       " '_ma',\n",
       " '_maî',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'maî',\n",
       " 'maît',\n",
       " 'a',\n",
       " 'aî',\n",
       " 'aît',\n",
       " 'aîtr',\n",
       " 'î',\n",
       " 'ît',\n",
       " 'îtr',\n",
       " 'ître',\n",
       " 't',\n",
       " 'tr',\n",
       " 'tre',\n",
       " 'tre_',\n",
       " 'r',\n",
       " 're',\n",
       " 're_',\n",
       " 'e',\n",
       " 'e_',\n",
       " '_d',\n",
       " '_di',\n",
       " '_dit',\n",
       " 'd',\n",
       " 'di',\n",
       " 'dit',\n",
       " 'dit_',\n",
       " 'i',\n",
       " 'it',\n",
       " 'it_',\n",
       " 't',\n",
       " 't_',\n",
       " '_o',\n",
       " '_on',\n",
       " '_on_',\n",
       " 'o',\n",
       " 'on',\n",
       " 'on_',\n",
       " 'n',\n",
       " 'n_',\n",
       " '_l',\n",
       " '_le',\n",
       " '_le_',\n",
       " 'l',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'e',\n",
       " 'e_',\n",
       " '_m',\n",
       " '_ma',\n",
       " '_mal',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mal',\n",
       " 'malh',\n",
       " 'a',\n",
       " 'al',\n",
       " 'alh',\n",
       " 'alhe',\n",
       " 'l',\n",
       " 'lh',\n",
       " 'lhe',\n",
       " 'lheu',\n",
       " 'h',\n",
       " 'he',\n",
       " 'heu',\n",
       " 'heur',\n",
       " 'e',\n",
       " 'eu',\n",
       " 'eur',\n",
       " 'eur_',\n",
       " 'u',\n",
       " 'ur',\n",
       " 'ur_',\n",
       " 'r',\n",
       " 'r_',\n",
       " '_e',\n",
       " '_es',\n",
       " '_est',\n",
       " 'e',\n",
       " 'es',\n",
       " 'est',\n",
       " 'est_',\n",
       " 's',\n",
       " 'st',\n",
       " 'st_',\n",
       " 't',\n",
       " 't_',\n",
       " '_q',\n",
       " '_qu',\n",
       " \"_qu'\",\n",
       " 'q',\n",
       " 'qu',\n",
       " \"qu'\",\n",
       " \"qu'i\",\n",
       " 'u',\n",
       " \"u'\",\n",
       " \"u'i\",\n",
       " \"u'il\",\n",
       " \"'\",\n",
       " \"'i\",\n",
       " \"'il\",\n",
       " \"'il_\",\n",
       " 'i',\n",
       " 'il',\n",
       " 'il_',\n",
       " 'l',\n",
       " 'l_',\n",
       " '_t',\n",
       " '_tu',\n",
       " '_tue',\n",
       " 't',\n",
       " 'tu',\n",
       " 'tue',\n",
       " 'tue_',\n",
       " 'u',\n",
       " 'ue',\n",
       " 'ue_',\n",
       " 'e',\n",
       " 'e_',\n",
       " '_s',\n",
       " '_se',\n",
       " '_ses',\n",
       " 's',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'ses_',\n",
       " 'e',\n",
       " 'es',\n",
       " 'es_',\n",
       " 's',\n",
       " 's_',\n",
       " '_é',\n",
       " '_él',\n",
       " '_élè',\n",
       " 'é',\n",
       " 'él',\n",
       " 'élè',\n",
       " 'élèv',\n",
       " 'l',\n",
       " 'lè',\n",
       " 'lèv',\n",
       " 'lève',\n",
       " 'è',\n",
       " 'èv',\n",
       " 'ève',\n",
       " 'èves',\n",
       " 'v',\n",
       " 've',\n",
       " 'ves',\n",
       " 'e',\n",
       " 'es',\n",
       " 's']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_extractor(sent):\n",
    "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
    "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
    "\n",
    "ngram_extractor(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
